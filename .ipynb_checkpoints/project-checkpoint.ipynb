{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "############# LAVORAZIONE DATI\n",
    "- suddivido le immagini in cartelle per categoria\n",
    "- sistemo il dataframe con sole nomi immagini e dx\n",
    "- per ogni dx:\n",
    "\t- calcolo quanto augmentation devo fare\n",
    "            per ogni categoria:\n",
    "                x = n° immagini in categoria\n",
    "                y = n° immagini nv\n",
    "                n° repliche per immagine = ceil((y-x)/x)\n",
    "\t- per ogni immagine:\n",
    "\t\t- la carico in memoria\n",
    "\t\t- flow, salvando le augmented\n",
    "\t\t- chiudo l'immagine (es. del x)\n",
    "- aggiungo al dataframe la nuova tupla di augmented\n",
    "- salvo il nuovo dataframe                            (fatto fin qua, sta in data_augmentation)\n",
    "\n",
    "################### TRAINING\n",
    "- copia temporanea dataset completo tmp\n",
    "- dichiarazione size per il batch\n",
    "- se esiste carico il modello del classificatore (a mano, tanto va fatto una sola volta)\n",
    "- fin quando dim(tmp) >= size:\n",
    "    - prendo size campioni random da tmp\n",
    "    - train_test_split dei campioni\n",
    "    - fit del modello con questi campioni:\n",
    "        - carico le immagini\n",
    "        - converto in array\n",
    "        - fit()    (DA STUDIARE BENE I PARAMETRI, https://keras.io/models/sequential/)\n",
    "        - dealloco le immagini\n",
    "    - salvo il modello (dovrebbe funzionare perché non lo dealloco mai)\n",
    "    - cancello i campioni da tmp\n",
    "    - aggiorno dim\n",
    "    - dealloco i campioni (dataframe)\n",
    "========================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "#import matplotlib.pyplot as plt\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#import sklearn as skl\n",
    "import sklearn.preprocessing as preproc\n",
    "import numpy as np\n",
    "import keras as k\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, ZeroPadding2D, Flatten, Dense, Dropout, MaxPool2D\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "#from time import time\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import math\n",
    "import shutil\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### variabili globali\n",
    "#### GIUSEPPE\n",
    "image_path = '..\\skin-cancer-mnist-ham10000\\HAM10000_images_part_1'\n",
    "#### FABIO\n",
    "# image_path = '..\\skin-cancer-mnist-ham10000\\HAM10000_images_part_1'\n",
    "csv_path = 'HAM10000_metadata.csv'\n",
    "csv_completo = 'dataframe_completo.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## API varie\n",
    "def lab_encode(dataset, le):#Ho una sola colonna! \n",
    "    new_ds = dataset\n",
    "    new_ds[\"dx\"] = le.fit_transform(dataset[\"dx\"])  \n",
    "    return new_ds\n",
    "\n",
    "def lab_decode(dataset, le):\n",
    "    new_ds = dataset\n",
    "    new_ds[\"dx\"] = le.inverse_transform(dataset[\"dx\"])  \n",
    "    return new_ds\n",
    "\n",
    "# conteggio statistiche dataset\n",
    "def stats(dataset):\n",
    "    labels = set(dataset['dx'])\n",
    "    for label in labels:\n",
    "        print(str(label), ':\\t', str(dataset[dataset['dx']==label]['dx'].count()))\n",
    "        #if label != \"nv\":\n",
    "        #    x = dataset[dataset['dx']==label]['dx'].count()\n",
    "        #    y = dataset[dataset['dx']==\"nv\"]['dx'].count()\n",
    "        #    print(\"ugmented per image: \", str(math.floor((y-x)/x)))\n",
    "        x = dataset[dataset['dx']==label]['dx'].count()\n",
    "        y = dataset[dataset['dx']==5]['dx'].count()\n",
    "        value = math.floor((y-x)/x)\n",
    "        print(\"ugmented per image: \", str(value))\n",
    "        aug_size[label] = value\n",
    "        #print (label)\n",
    "\n",
    "        \n",
    "        \n",
    "def new_classifier():\n",
    "    input_img = Input(shape=(450, 600, 3))  # 3x600x450 image RGB \n",
    "    #print (tf.shape(input_img))\n",
    "    x = Conv2D(20, (5, 5), activation='relu', padding='same')(input_img) # 20x450x600\n",
    "    #print (tf.shape(x))\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x) # 20x225x300\n",
    "    #print (tf.shape(x))\n",
    "    x = Conv2D(140, (3, 3), activation='relu', padding='same')(x) # 140x75x100\n",
    "    #print (tf.shape(x))\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x) # 40x38x50\n",
    "    #print (tf.shape(x))\n",
    "    x = Conv2D(50, (3, 3), activation='relu', padding='same')(x) # 50x38x50\n",
    "    #print (tf.shape(x))\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x) # 50x19x25\n",
    "    #print (tf.shape(x))\n",
    "    x = Flatten()(x) # qui la x diventa monodimensionale\n",
    "    #print (tf.shape(x))\n",
    "    # da qualche parte ci va il numero delle label?\n",
    "    encoded = Dense(7, activation='softmax')(x)\n",
    "    classifier = k.models.Model(input_img, encoded)   #questo è il nostro base_estimator, compile configura per il training\n",
    "    classifier.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "    return classifier\n",
    "\n",
    "def new_classifier2():\n",
    "    # Set the CNN model \n",
    "    input_shape = (450, 600, 3)\n",
    "    num_classes = 7\n",
    "    \n",
    "    model = k.models.Sequential()\n",
    "    model.add(Conv2D(20, kernel_size=(5, 5),activation='relu',padding = 'Same', input_shape = input_shape))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "    model.add(Conv2D(140,kernel_size=(3, 3), activation='relu',padding = 'Same'))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "    model.add(Conv2D(50,kernel_size=(3, 3), activation='relu',padding = 'Same'))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "def new_classifier3():\n",
    "    # Set the CNN model \n",
    "    input_shape = (150, 200, 3)\n",
    "    num_classes = 7\n",
    "    \n",
    "    model = k.models.Sequential()\n",
    "    model.add(Conv2D(20, kernel_size=(5, 5),activation='relu',padding = 'Same', input_shape = input_shape))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "    model.add(Conv2D(140,kernel_size=(3, 3), activation='relu',padding = 'Same'))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "    model.add(Conv2D(50,kernel_size=(3, 3), activation='relu',padding = 'Same'))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "def initialize_dataset():\n",
    "    dataset = pd.read_csv(csv_path, encoding = \"ISO-8859-1\")\n",
    "    labels = set(dataset[\"dx\"])\n",
    "    new_ds = pd.DataFrame(dataset)\n",
    "    columns=[\"lesion_id\", \"dx_type\", \"age\", \"sex\", \"localization\"]\n",
    "    new_ds = new_ds.drop(columns=columns, axis=0)\n",
    "    return new_ds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ INIZIALIZZAZIONE\n",
    "#creazione dataframe\n",
    "ds, labels = initialize_dataset()\n",
    "# encoding delle labels (dx)\n",
    "encoder = preproc.LabelEncoder()\n",
    "encoded = lab_encode(ds, encoder)\n",
    "encoded = encoded.sort_index()\n",
    "\n",
    "aug_size = [0,0,0,0,0,0,0]\n",
    "\n",
    "# Create a data generator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=180,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    #brightness_range=(0.9,1.1),\n",
    "    fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### creazione augmented\n",
    "\n",
    "# note that we are not augmenting class 'nv'\n",
    "#class_list = ['0','1','2','3','4','6']\n",
    "class_list = ['3','4','6']\n",
    "\n",
    "aug_dir = '..\\\\aug_dir'\n",
    "img_dir = os.path.join(aug_dir, 'img_dir')    \n",
    "\n",
    "\n",
    "# Choose a class\n",
    "for img_class in class_list:\n",
    "    \n",
    "    # creazione cartelle contenenti le augmented\n",
    "    dst_class_dir = os.path.join(img_dir, img_class)\n",
    "    os.mkdir(dst_class_dir)\n",
    "    \n",
    "    \n",
    "    # list all images in that directory\n",
    "    src_class_dir = os.path.join('..\\\\skin-cancer-mnist-ham10000\\\\HAM10000_images_part_1', img_class)\n",
    "    img_list = os.listdir(src_class_dir) # qui le immagini sono già nelle proprie cartelle\n",
    "    print('#img: '+ str(len(img_list)))\n",
    "    \n",
    "    # per ogni immagine devo creare le augmented\n",
    "    for fname in img_list:\n",
    "        fpath = os.path.join(src_class_dir, fname)\n",
    "        img = Image.open(fpath)  # this is a PIL image\n",
    "        x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "        x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "        \n",
    "        prefix = fname.replace(\".jpg\", \"\")\n",
    "        # the .flow() command below generates batches of randomly transformed images\n",
    "        # and saves the results to the `preview/` directory\n",
    "        i = 0\n",
    "        for batch in datagen.flow(x, batch_size=1,\n",
    "                                  save_to_dir=dst_class_dir, save_prefix=prefix , save_format='jpg'):\n",
    "            i += 1\n",
    "            if i > aug_size[int(img_class)]:\n",
    "                break  # otherwise the generator would loop indefinitely\n",
    "        \n",
    "        del x\n",
    "        img.close()\n",
    "    # run the generator and create about 6000 augmented images\n",
    "    #for i in range(0,num_batches):\n",
    "    #\n",
    "    #    imgs, labels = next(aug_datagen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## sistemazione nuovo dataset\n",
    "aug_dir = '..\\\\aug_dir'\n",
    "img_dir = os.path.join(aug_dir, 'img_dir') \n",
    "classes = os.listdir(img_dir)\n",
    "\n",
    "columns = encoded.columns\n",
    "tupla = pd.DataFrame(columns=columns)\n",
    "index = 10015\n",
    "\n",
    "for elem1 in classes:\n",
    "    class_dir = os.path.join(img_dir, elem1)\n",
    "    images = os.listdir(class_dir)\n",
    "    for elem2 in images:\n",
    "        elem2 = elem2.replace('.jpg', '')\n",
    "        tupla = tupla.append({'image_id': elem2, 'dx': int(elem1)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvataggio (punto 1.2)\n",
    "#dataset.to_csv('Pokemon_clean.csv', index=False) \n",
    "encoded = encoded.append(tupla)\n",
    "encoded.to_csv('dataframe_completo.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# da provare\n",
    "X = encoded[\"pixels\"]\n",
    "y = pd.get_dummies(encoded[\"dx\"])\n",
    "#bag_class = ensemble()\n",
    "X = np.asarray(X.tolist())\n",
    "y = np.asarray(y)\n",
    "\n",
    "# splitting test and validation\n",
    "X_train, X_test1, y_train, y_test1 = train_test_split(X, y, test_size=0.30)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test1, y_test1, test_size=0.40)\n",
    "\n",
    "\n",
    "classifier = new_classifier3()\n",
    "# STUDIARE BENE IL FIT\n",
    "classifier.fit(X_train, y_train, epochs=1, batch_size=100, shuffle=True, validation_data=(X_val, y_val)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "datagen.fit(X_train)\n",
    "\n",
    "cl2 = new_classifier3()\n",
    "# fits the model on batches with real-time data augmentation:\n",
    "cl2.fit_generator(datagen.flow(X_train, y_train, batch_size=32),\n",
    "                    steps_per_epoch=len(X_train) / 32, epochs=1)\n",
    "\n",
    "# here's a more \"manual\" example\n",
    "for e in range(2):\n",
    "    print('Epoch', e)\n",
    "    batches = 0\n",
    "    for x_batch, y_batch in datagen.flow(X_train, y_train, batch_size=20):\n",
    "        cl2.fit(x_batch, y_batch)\n",
    "        batches += 1\n",
    "        if batches >= len(X_train) / 20:\n",
    "            print('fine batch ', batches)\n",
    "            # we need to break the loop by hand because\n",
    "            # the generator loops indefinitely\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### TRAINING\n",
    "#- copia temporanea dataset completo tmp\n",
    "#- dichiarazione size per il batch\n",
    "#- se esiste carico il modello del classificatore (a mano, tanto va fatto una sola volta)\n",
    "#- fin quando dim(tmp) >= size:\n",
    "#    - prendo size campioni random da tmp\n",
    "#    - train_test_split dei campioni                   FERMO QUI, implementare funzione che da array di nomi ritorna l'array di pixels delle immagini\n",
    "#    - fit del modello con questi campioni:\n",
    "#        - carico le immagini\n",
    "#        - converto in array\n",
    "#        - fit()    (DA STUDIARE BENE I PARAMETRI, https://keras.io/models/sequential/)\n",
    "#        - dealloco le immagini\n",
    "#    - salvo il modello (dovrebbe funzionare perché non lo dealloco mai)\n",
    "#    - cancello i campioni da tmp\n",
    "#    - aggiorno dim\n",
    "#    - dealloco i campioni (dataframe)\n",
    "\n",
    "dataset = pd.read_csv(csv_completo, encoding = \"ISO-8859-1\")\n",
    "batch = 500\n",
    "size = len(dataset)\n",
    "while size >= batch:\n",
    "    # To get 3 random rows \n",
    "    samples = pd.DataFrame(dataset)\n",
    "    samples = samples.sample(n = batch)\n",
    "    # splitting test and validation\n",
    "    X_train, X_test1, y_train, y_test1 = train_test_split(samples[\"image_id\"], samples[\"dx\"], test_size=0.30)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_test1, y_test1, test_size=0.40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>dx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39127</th>\n",
       "      <td>ISIC_0032726_0_7692</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28833</th>\n",
       "      <td>ISIC_0025594_0_8233</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45877</th>\n",
       "      <td>ISIC_0031759_0_6160</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23360</th>\n",
       "      <td>ISIC_0027042_0_5861</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33968</th>\n",
       "      <td>ISIC_0033626_0_699</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6936</th>\n",
       "      <td>ISIC_0025370</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23291</th>\n",
       "      <td>ISIC_0026961_0_5458</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18089</th>\n",
       "      <td>ISIC_0026766_0_7519</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4675</th>\n",
       "      <td>ISIC_0026748</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18403</th>\n",
       "      <td>ISIC_0027120_0_9891</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  image_id  dx\n",
       "39127  ISIC_0032726_0_7692   4\n",
       "28833  ISIC_0025594_0_8233   3\n",
       "45877  ISIC_0031759_0_6160   6\n",
       "23360  ISIC_0027042_0_5861   2\n",
       "33968   ISIC_0033626_0_699   3\n",
       "...                    ...  ..\n",
       "6936          ISIC_0025370   5\n",
       "23291  ISIC_0026961_0_5458   2\n",
       "18089  ISIC_0026766_0_7519   1\n",
       "4675          ISIC_0026748   5\n",
       "18403  ISIC_0027120_0_9891   1\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
