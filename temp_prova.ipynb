{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import sklearn as skl\n",
    "import sklearn.preprocessing as preproc\n",
    "import numpy as np\n",
    "import keras as k\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, ZeroPadding2D, Flatten, Dense, Dropout, MaxPool2D\n",
    "from keras.models import Sequential\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "#from time import time\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import BatchNormalization\n",
    "import math\n",
    "import shutil\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#### variabili globali\n",
    "#### GIUSEPPE\n",
    "#image_path = '..\\skin-cancer-mnist-ham10000\\HAM10000_images_part_1'\n",
    "image_path = '..\\\\..\\\\Untitled_Folder'\n",
    "#### FABIO\n",
    "# image_path = '..\\skin-cancer-mnist-ham10000\\HAM10000_images_part_1'\n",
    "csv_path = 'HAM10000_metadata.csv'\n",
    "csv_completo = 'dataframe_completo.csv'\n",
    "csv_completo_2 = 'dataframe_completo_2.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(csv_completo_2, encoding = \"ISO-8859-1\")\n",
    "batch = 1000\n",
    "\n",
    "def my_train_batch(dataset, batch, epochs, classifier):\n",
    "    size = len(dataset)\n",
    "    epoch=0\n",
    "    cl1 = k.models.load_model(classifier)\n",
    "    # train del classificatore un batch alla volta\n",
    "    while size >= batch:\n",
    "        # To get n random rows \n",
    "        samples = dataset.sample(n = batch)\n",
    "        # splitting test and validation\n",
    "        X_train, X_val, y_train, y_val = train_test_split(samples[\"image_id\"], samples[\"dx\"], test_size=0.30)\n",
    "        \n",
    "        train_img = np.asarray(load_images(X_train))\n",
    "        val_img = np.asarray(load_images(X_val))\n",
    "        \n",
    "        #1hot encoder\n",
    "        enc = OneHotEncoder(sparse=False)\n",
    "        y_train_shaped = np.array(y_train).reshape(-1,1)\n",
    "        y_val_shaped = np.array(y_val).reshape(-1,1)\n",
    "        enc.fit(y_train_shaped)\n",
    "        y_train_one = enc.transform(y_train_shaped)\n",
    "        y_val_one = enc.transform(y_val_shaped)\n",
    "        \n",
    "        cl1.fit(train_img, y_train_one, epochs=epochs, shuffle=True, validation_data=(val_img, y_val_one), verbose=1)\n",
    "        \n",
    "        #pulizia memoria e reset per il prossimo ciclo\n",
    "        del X_train, y_train_one, X_val, y_val_one, train_img, val_img\n",
    "        dataset = dataset.drop(samples.index)\n",
    "        size = len(dataset)\n",
    "        del samples, enc\n",
    "        epoch = epoch + 1\n",
    "        print(\"fine epoca \" + str(epoch) + \";\\trestano \" + str(size) + \" campioni nel dataset\")\n",
    "    cl1.save(classifier)\n",
    "    return\n",
    "\n",
    "# dal dataframe ne estrae le immagini in un array, serve nel batch\n",
    "def load_images(array):\n",
    "    images = list()\n",
    "    #print(array.iloc[0])\n",
    "    for index in range(0,len(array)):\n",
    "        #print(index.type())\n",
    "        img_id = array.iloc[index]\n",
    "        elem = os.path.join(image_path, img_id)\n",
    "        elem = elem + \".jpg\"\n",
    "        img = cv2.imread(elem)\n",
    "        # TODO va fatto un reshape, non capisco perchÃ©\n",
    "        images.append(np.asarray(img)) #########################################################################################\n",
    "    return images\n",
    "\n",
    "def new_classifier3():\n",
    "    # Set the CNN model \n",
    "    input_shape = (150, 200, 3)\n",
    "    num_classes = 7\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(20, kernel_size=(5, 5),activation='relu',padding = 'Same', input_shape = input_shape))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(140,kernel_size=(3, 3), activation='relu',padding = 'Same'))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(50,kernel_size=(3, 3), activation='relu',padding = 'Same'))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "def kaggle_classifier():\n",
    "    # Set the CNN model \n",
    "    # my CNN architechture is In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Out\n",
    "    input_shape = (150, 200, 3)\n",
    "    num_classes = 7\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, kernel_size=(3, 3),activation='relu',padding = 'Same',input_shape=input_shape))\n",
    "    model.add(Conv2D(16,kernel_size=(3, 3), activation='relu',padding = 'Same',))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3), activation='relu',padding = 'Same'))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu',padding = 'Same'))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.40))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.summary()\n",
    "    model.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 150, 200, 20)      1520      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 75, 100, 20)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 75, 100, 20)       80        \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 75, 100, 140)      25340     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 37, 50, 140)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 37, 50, 140)       560       \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 37, 50, 50)        63050     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 18, 25, 50)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 18, 25, 50)        200       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 18, 25, 50)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 22500)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               2880128   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 2,971,781\n",
      "Trainable params: 2,971,361\n",
      "Non-trainable params: 420\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### inizializzazione classificatore sul disco\n",
    "cl1 = new_classifier3()\n",
    "cl1.save('cl1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass = int(len(dataset)*0.7)\n",
    "samples = dataset.sample(n=mass)\n",
    "my_train_batch(dataset=samples, batch=10000, epochs=20, classifier = 'classifier1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### controllo dimensioni immagini nella cartella\n",
    "legit = (200, 150)\n",
    "i = 0\n",
    "for elem in dataset[\"image_id\"]:\n",
    "    fname = elem + \".jpg\"\n",
    "    opened = Image.open(os.path.join(image_path, fname))\n",
    "    if opened.size != legit:\n",
    "        print(opened.size)\n",
    "    opened.close()\n",
    "    i = i+1\n",
    "    if i % 5000 == 0:\n",
    "        print(str(i) + \" iterate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 150, 200, 20)      1520      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 75, 100, 20)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 75, 100, 20)       80        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 75, 100, 140)      25340     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 37, 50, 140)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 37, 50, 140)       560       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 37, 50, 50)        63050     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 18, 25, 50)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 18, 25, 50)        200       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 18, 25, 50)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 22500)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               2880128   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 2,971,781\n",
      "Trainable params: 2,971,361\n",
      "Non-trainable params: 420\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### inizializzazione classificatore sul disco\n",
    "cl_name = 'training_batch_cl1.h5'\n",
    "\n",
    "cl1 = new_classifier3()\n",
    "cl1.save(cl_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4200 samples, validate on 1800 samples\n",
      "Epoch 1/5\n",
      "4200/4200 [==============================] - 172s 41ms/step - loss: 0.5001 - val_loss: 0.4817\n",
      "Epoch 2/5\n",
      "4200/4200 [==============================] - 179s 43ms/step - loss: 0.3069 - val_loss: 0.3369\n",
      "Epoch 3/5\n",
      "4200/4200 [==============================] - 179s 43ms/step - loss: 0.2849 - val_loss: 0.4828\n",
      "Epoch 4/5\n",
      "4200/4200 [==============================] - 178s 42ms/step - loss: 0.2627 - val_loss: 1.4253\n",
      "Epoch 5/5\n",
      "4200/4200 [==============================] - 179s 43ms/step - loss: 0.2393 - val_loss: 0.6021\n",
      "fine epoca 1;\trestano 27568 campioni nel dataset\n",
      "Train on 4200 samples, validate on 1800 samples\n",
      "Epoch 1/5\n",
      "4200/4200 [==============================] - 179s 43ms/step - loss: 0.2676 - val_loss: 0.3051\n",
      "Epoch 2/5\n",
      "4200/4200 [==============================] - 3056s 728ms/step - loss: 0.2398 - val_loss: 3.5680\n",
      "Epoch 3/5\n",
      "4200/4200 [==============================] - 155s 37ms/step - loss: 0.2238 - val_loss: 1.3627\n",
      "Epoch 4/5\n",
      "4200/4200 [==============================] - 163s 39ms/step - loss: 0.1962 - val_loss: 0.8534\n",
      "Epoch 5/5\n",
      "4200/4200 [==============================] - 163s 39ms/step - loss: 0.1780 - val_loss: 1.1831\n",
      "fine epoca 2;\trestano 21568 campioni nel dataset\n",
      "Train on 4200 samples, validate on 1800 samples\n",
      "Epoch 1/5\n",
      "4200/4200 [==============================] - 170s 41ms/step - loss: 0.2640 - val_loss: 0.4427\n",
      "Epoch 2/5\n",
      "4200/4200 [==============================] - 169s 40ms/step - loss: 0.2253 - val_loss: 0.3728\n",
      "Epoch 3/5\n",
      "4200/4200 [==============================] - 169s 40ms/step - loss: 0.1974 - val_loss: 0.3178\n",
      "Epoch 4/5\n",
      "4200/4200 [==============================] - 170s 40ms/step - loss: 0.1760 - val_loss: 0.2860\n",
      "Epoch 5/5\n",
      "4200/4200 [==============================] - 170s 40ms/step - loss: 0.1498 - val_loss: 0.8030\n",
      "fine epoca 3;\trestano 15568 campioni nel dataset\n",
      "Train on 4200 samples, validate on 1800 samples\n",
      "Epoch 1/5\n",
      "4200/4200 [==============================] - 170s 41ms/step - loss: 0.2440 - val_loss: 0.2483\n",
      "Epoch 2/5\n",
      "4200/4200 [==============================] - 172s 41ms/step - loss: 0.1970 - val_loss: 0.6959\n",
      "Epoch 3/5\n",
      "4200/4200 [==============================] - 176s 42ms/step - loss: 0.1669 - val_loss: 0.4300\n",
      "Epoch 4/5\n",
      "4200/4200 [==============================] - 174s 41ms/step - loss: 0.1356 - val_loss: 0.8006\n",
      "Epoch 5/5\n",
      "4200/4200 [==============================] - 173s 41ms/step - loss: 0.1139 - val_loss: 0.2875\n",
      "fine epoca 4;\trestano 9568 campioni nel dataset\n",
      "Train on 4200 samples, validate on 1800 samples\n",
      "Epoch 1/5\n",
      "4200/4200 [==============================] - 176s 42ms/step - loss: 0.2435 - val_loss: 0.5503\n",
      "Epoch 2/5\n",
      "4200/4200 [==============================] - 174s 42ms/step - loss: 0.1918 - val_loss: 0.3527\n",
      "Epoch 3/5\n",
      "4200/4200 [==============================] - 177s 42ms/step - loss: 0.1588 - val_loss: 0.3483\n",
      "Epoch 4/5\n",
      "4200/4200 [==============================] - 174s 41ms/step - loss: 0.1323 - val_loss: 0.6655\n",
      "Epoch 5/5\n",
      "4200/4200 [==============================] - 176s 42ms/step - loss: 0.1076 - val_loss: 0.8162\n",
      "fine epoca 5;\trestano 3568 campioni nel dataset\n"
     ]
    }
   ],
   "source": [
    "mass = int(len(dataset)*0.7)\n",
    "samples = dataset.sample(n=mass)\n",
    "my_train_batch(dataset=samples, batch=6000, epochs=5, classifier = cl_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl1.save(cl_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.asarray(load_images(dataset[\"image_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-4a384bfe42b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0my_val_one\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val_shaped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mcl1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_one\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val_one\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'epochs' is not defined"
     ]
    }
   ],
   "source": [
    "X_train1, X_test, y_train1, y_test = train_test_split(images, dataset[\"dx\"], test_size=0.30)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train1, y_train1, test_size=0.30)\n",
    "\n",
    "#1hot encoder\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "y_train_shaped = np.array(y_train).reshape(-1,1)\n",
    "y_val_shaped = np.array(y_val).reshape(-1,1)\n",
    "enc.fit(y_train_shaped)\n",
    "y_train_one = enc.transform(y_train_shaped)\n",
    "y_val_one = enc.transform(y_val_shaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23497 samples, validate on 10071 samples\n",
      "Epoch 1/5\n",
      "23497/23497 [==============================] - 1092s 46ms/step - loss: 0.3254 - val_loss: 0.3121\n",
      "Epoch 2/5\n",
      "23497/23497 [==============================] - 1058s 45ms/step - loss: 0.2507 - val_loss: 0.3938\n",
      "Epoch 3/5\n",
      "23497/23497 [==============================] - 1020s 43ms/step - loss: 0.2261 - val_loss: 0.3642\n",
      "Epoch 4/5\n",
      "23497/23497 [==============================] - 1023s 44ms/step - loss: 0.2036 - val_loss: 0.3431\n",
      "Epoch 5/5\n",
      "23497/23497 [==============================] - 1026s 44ms/step - loss: 0.1867 - val_loss: 0.3544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x28ce1781710>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl1.fit(X_train, y_train_one, epochs=5, shuffle=True, validation_data=(X_val, y_val_one), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl1.save(\"TRAINED_nc3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
