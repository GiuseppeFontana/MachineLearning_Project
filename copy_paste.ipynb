{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#creazione split\n",
    "feat_train, feat_test, target_train, target_test = train_test_split(X, y, test_size=0.20, random_state=28) \n",
    "smote = SMOTENC(sampling_strategy='dict', categorical_features=['bkl', 'vasc'])\n",
    "X_sm, y_sm = smote.fit_sample(X,y)\n",
    "len(X)\n",
    "len(X_sm)\n",
    "\n",
    "print(type(X))\n",
    "\n",
    "\n",
    "\n",
    "os.replace(\"path/to/current/file.foo\", \"path/to/new/destination/for/file.foo\")\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# Fit the model\n",
    "epochs = 50 \n",
    "batch_size = 10\n",
    "history = model.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size),\n",
    "                              epochs = epochs, validation_data = (x_validate,y_validate),\n",
    "                              verbose = 1, steps_per_epoch=x_train.shape[0] // batch_size\n",
    "                              , callbacks=[learning_rate_reduction])\n",
    "                              \n",
    "                              \n",
    "                              \n",
    "                              \n",
    "########################### matteo\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "y_train\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "y_train_shaped = np.array(y_train).reshape(-1,1)\n",
    "y_test_shaped = np.array(y_test).reshape(-1,1)\n",
    "y_validate_shaped = np.array(y_validate).reshape(-1,1)\n",
    "\n",
    "enc.fit(y_train_shaped)\n",
    "\n",
    "y_train_one = enc.transform(y_train_shaped)\n",
    "y_test_one = enc.transform(y_test_shaped)\n",
    "y_validate_one = enc.transform(y_validate_shaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# 1hot matteo\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "y_train\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "y_train_shaped = np.array(y_train).reshape(-1,1)\n",
    "y_test_shaped = np.array(y_test).reshape(-1,1)\n",
    "y_validate_shaped = np.array(y_validate).reshape(-1,1)\n",
    "\n",
    "enc.fit(y_train_shaped)\n",
    "\n",
    "y_train_one = enc.transform(y_train_shaped)\n",
    "y_test_one = enc.transform(y_test_shaped)\n",
    "y_validate_one = enc.transform(y_validate_shaped)\n",
    "\n",
    "\n",
    "########## resize immagini fabio\n",
    "path=\"pathImage\"\n",
    "size=(200,100)\n",
    "img=(Image.open(path))\n",
    "img_resize_array=np.asarray(Image.open(path).resize(size))\n",
    "img_resize=(Image.open(path).resize(size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - carica in memoria immagini e label\n",
    "# - 1-hot encoding\n",
    "# - implementazione modello tramite ensamble (nel classificatore va incluso il numero di labels)\n",
    "# - train e test usando il bagging, ma prima bisogna fare SMOTE su vasc e bkl?\n",
    "\n",
    "###############################\n",
    "#############################\n",
    "###### NON SERVONO\n",
    "# carica tutto in un nuovo dataframe\n",
    "#def load_pictures(path, dataset):\n",
    "#    col=[\"lesion_id\" ,\"image_id\",\"dx\",\"dx_type\" ,\"age\" ,\"sex\" ,\"localization\",\"path\",\"pixels\"]\n",
    "#    dataset.insert(7,\"path\",None)\n",
    "#    dataset.insert(8,\"pixels\",None)\n",
    "#    data=pd.DataFrame(columns=col)\n",
    "#    helper = pd.DataFrame(columns=col)\n",
    "#    for _,_,files in os.walk(path+'\\\\'):\n",
    "#        for file in files: \n",
    "#            file_replace = file.replace(\".jpg\", \"\") #tagliare il .jpg che non compare nel dataset\n",
    "#            path2=path+\"\\\\\"+file #path completo di un file\n",
    "#            helper=dataset.loc[dataset[\"image_id\"]==file_replace] #tupla del file\n",
    "#            helper[\"path\"]=path2\n",
    "#            col=[\"lesion_id\" ,\"image_id\",\"dx\",\"dx_type\" ,\"age\" ,\"sex\" ,\"localization\"]\n",
    "#            data = data.append(helper)\n",
    "#    data[\"pixels\"]=data['path'].map(lambda x: np.asarray(Image.open(x).resize((200,150)))) #associazione ai pixel\n",
    "#    return data\n",
    "#\n",
    "#\n",
    "#def onehot_encode(dataset):\n",
    "#    dataset_LabEncoded = lab_encode(dataset)\n",
    "#    x=np.array([dataset_LabEncoded[\"dx\"]]) \n",
    "#    y_onehot = ohe.fit_transform(x.reshape(-1,1))\n",
    "#    return y_onehot\n",
    "#\n",
    "#\n",
    "## si prende tutto l'array di risultati\n",
    "#def reverse_onehot(vector):\n",
    "#    res = ohe.inverse_transform(vector)\n",
    "#    return res\n",
    "#\n",
    "#\n",
    "## conteggio statistiche dataset\n",
    "#def stats(dataset):\n",
    "#    labels = set(dataset['dx'])\n",
    "#    for label in labels:\n",
    "#        print(str(label), ':\\t', str(dataset[dataset['dx']==label]['dx'].count()))\n",
    "#        #print (label)\n",
    "#        \n",
    "#def ensemble():\n",
    "#    #da settare \n",
    "#    #    base_estimator il classificatore come CNN,\n",
    "#    #    n_estimators,\n",
    "#    #    max_samples Ã¨ il numero di elementi nei training set,\n",
    "#    #    bootstrap se lo vogliamo o no (booleano),\n",
    "#    #    n_jobs per il parallelismo,\n",
    "#    classifier = new_classifier2()\n",
    "#    bagging = BaggingClassifier(base_estimator=classifier, n_estimators=1, \n",
    "#                                bootstrap=True, n_jobs=4)\n",
    "#    return bagging\n",
    "###################################\n",
    "####################################\n",
    "\n",
    "\n",
    "# da provare\n",
    "#X = encoded[\"pixels\"]\n",
    "#y = encoded[\"dx\"]\n",
    "#bag_class = ensemble()\n",
    "#X = np.asarray(X)\n",
    "#y = np.asarray(y)\n",
    "#\n",
    "#dataset_size = len(X)\n",
    "#reshaped = np.reshape(X,(dataset_size,-1))\n",
    "#params = bag_class.get_params()\n",
    "#bag_class.fit(reshaped, y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
