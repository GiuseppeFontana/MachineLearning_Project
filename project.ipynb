{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "############# LAVORAZIONE DATI\n",
    "- suddivido le immagini in cartelle per categoria\n",
    "- sistemo il dataframe con sole nomi immagini e dx\n",
    "- per ogni dx:\n",
    "\t- calcolo quanto augmentation devo fare\n",
    "            per ogni categoria:\n",
    "                x = n° immagini in categoria\n",
    "                y = n° immagini nv\n",
    "                n° repliche per immagine = ceil((y-x)/x)\n",
    "\t- per ogni immagine:\n",
    "\t\t- la carico in memoria\n",
    "\t\t- flow, salvando le augmented\n",
    "\t\t- chiudo l'immagine (es. del x)\n",
    "- aggiungo al dataframe la nuova tupla di augmented\n",
    "- salvo il nuovo dataframe\n",
    "\n",
    "################### TRAINING\n",
    "- copia temporanea dataset completo tmp\n",
    "- dichiarazione size per il batch\n",
    "- se esiste carico il modello del classificatore (a mano, tanto va fatto una sola volta)\n",
    "- fin quando dim(tmp) >= size:\n",
    "    - prendo size campioni random da tmp\n",
    "    - train_test_split dei campioni\n",
    "    - fit del modello con questi campioni:\n",
    "        - carico le immagini\n",
    "        - converto in array\n",
    "        - fit()    (DA STUDIARE BENE I PARAMETRI, https://keras.io/models/sequential/)\n",
    "        - dealloco le immagini\n",
    "    - salvo il modello (dovrebbe funzionare perché non lo dealloco mai)\n",
    "    - cancello i campioni da tmp\n",
    "    - aggiorno dim\n",
    "    - dealloco i campioni (dataframe)\n",
    "========================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import sklearn as skl\n",
    "import sklearn.preprocessing as preproc\n",
    "import numpy as np\n",
    "import keras as k\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, ZeroPadding2D, Flatten, Dense, Dropout, MaxPool2D\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "#from time import time\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import math\n",
    "import shutil\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### variabili globali\n",
    "#### GIUSEPPE\n",
    "image_path = '..\\skin-cancer-mnist-ham10000\\HAM10000_images_part_1'\n",
    "#### FABIO\n",
    "# image_path = '..\\skin-cancer-mnist-ham10000\\HAM10000_images_part_1'\n",
    "csv_path = 'HAM10000_metadata.csv'\n",
    "csv_completo = 'dataframe_completo.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## API varie\n",
    "def lab_encode(dataset, le):#Ho una sola colonna! \n",
    "    new_ds = dataset\n",
    "    new_ds[\"dx\"] = le.fit_transform(dataset[\"dx\"])  \n",
    "    return new_ds\n",
    "\n",
    "def lab_decode(dataset, le):\n",
    "    new_ds = dataset\n",
    "    new_ds[\"dx\"] = le.inverse_transform(dataset[\"dx\"])  \n",
    "    return new_ds\n",
    "\n",
    "# conteggio statistiche dataset\n",
    "def stats(dataset):\n",
    "    labels = set(dataset['dx'])\n",
    "    for label in labels:\n",
    "        print(str(label), ':\\t', str(dataset[dataset['dx']==label]['dx'].count()))\n",
    "        #if label != \"nv\":\n",
    "        #    x = dataset[dataset['dx']==label]['dx'].count()\n",
    "        #    y = dataset[dataset['dx']==\"nv\"]['dx'].count()\n",
    "        #    print(\"ugmented per image: \", str(math.floor((y-x)/x)))\n",
    "        x = dataset[dataset['dx']==label]['dx'].count()\n",
    "        y = dataset[dataset['dx']==5]['dx'].count()\n",
    "        value = math.floor((y-x)/x)\n",
    "        print(\"ugmented per image: \", str(value))\n",
    "        aug_size[label] = value\n",
    "        #print (label)\n",
    "        \n",
    "def new_classifier1():\n",
    "    input_img = Input(shape=(450, 600, 3))  # 3x600x450 image RGB \n",
    "    #print (tf.shape(input_img))\n",
    "    x = Conv2D(20, (5, 5), activation='relu', padding='same')(input_img) # 20x450x600\n",
    "    #print (tf.shape(x))\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x) # 20x225x300\n",
    "    #print (tf.shape(x))\n",
    "    x = Conv2D(140, (3, 3), activation='relu', padding='same')(x) # 140x75x100\n",
    "    #print (tf.shape(x))\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x) # 40x38x50\n",
    "    #print (tf.shape(x))\n",
    "    x = Conv2D(50, (3, 3), activation='relu', padding='same')(x) # 50x38x50\n",
    "    #print (tf.shape(x))\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x) # 50x19x25\n",
    "    #print (tf.shape(x))\n",
    "    x = Flatten()(x) # qui la x diventa monodimensionale\n",
    "    #print (tf.shape(x))\n",
    "    # da qualche parte ci va il numero delle label?\n",
    "    encoded = Dense(7, activation='softmax')(x)\n",
    "    classifier = k.models.Model(input_img, encoded)   #questo è il nostro base_estimator, compile configura per il training\n",
    "    #classifier.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "    classifier.compile(optimizer='adadelta', loss='categorical_crossentropy') # <- questo potrebbe risolvere\n",
    "    return classifier\n",
    "\n",
    "def new_classifier2():\n",
    "    # Set the CNN model \n",
    "    input_shape = (450, 600, 3)\n",
    "    num_classes = 7\n",
    "    \n",
    "    model = k.models.Sequential()\n",
    "    model.add(Conv2D(20, kernel_size=(5, 5),activation='relu',padding = 'Same', input_shape = input_shape))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "    model.add(Conv2D(140,kernel_size=(3, 3), activation='relu',padding = 'Same'))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "    model.add(Conv2D(50,kernel_size=(3, 3), activation='relu',padding = 'Same'))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "def new_classifier3():\n",
    "    # Set the CNN model \n",
    "    input_shape = (150, 200, 3)\n",
    "    num_classes = 7\n",
    "    \n",
    "    model = k.models.Sequential()\n",
    "    model.add(Conv2D(20, kernel_size=(5, 5),activation='relu',padding = 'Same', input_shape = input_shape))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "    model.add(Conv2D(140,kernel_size=(3, 3), activation='relu',padding = 'Same'))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "    model.add(Conv2D(50,kernel_size=(3, 3), activation='relu',padding = 'Same'))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "def initialize_dataset():\n",
    "    dataset = pd.read_csv(csv_path, encoding = \"ISO-8859-1\")\n",
    "    labels = set(dataset[\"dx\"])\n",
    "    new_ds = pd.DataFrame(dataset)\n",
    "    columns=[\"lesion_id\", \"dx_type\", \"age\", \"sex\", \"localization\"]\n",
    "    new_ds = new_ds.drop(columns=columns, axis=0)\n",
    "    return new_ds, labels\n",
    "\n",
    "# dal dataframe ne estrae le immagini in un array, serve nel batch\n",
    "def load_images(array):\n",
    "    images = list()\n",
    "    #print(array.iloc[0])\n",
    "    for index in range(0,len(array)):\n",
    "        #print(index.type())\n",
    "        img_id = array.iloc[index]\n",
    "        elem = os.path.join(\"..\\skin-cancer-mnist-ham10000\\HAM10000_images_part_1\", img_id)\n",
    "        elem = elem + \".jpg\"\n",
    "        img = cv2.imread(elem)\n",
    "        # TODO va fatto un reshape, non capisco perché\n",
    "        images.append(img)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ INIZIALIZZAZIONE\n",
    "#creazione dataframe\n",
    "ds, labels = initialize_dataset()\n",
    "# encoding delle labels (dx)\n",
    "encoder = preproc.LabelEncoder()\n",
    "encoded = lab_encode(ds, encoder)\n",
    "encoded = encoded.sort_index()\n",
    "\n",
    "aug_size = [0,0,0,0,0,0,0]\n",
    "\n",
    "# Create a data generator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=180,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    #brightness_range=(0.9,1.1),\n",
    "    fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### creazione augmented\n",
    "\n",
    "# note that we are not augmenting class 'nv'\n",
    "#class_list = ['0','1','2','3','4','6']\n",
    "class_list = ['3','4','6']\n",
    "\n",
    "aug_dir = '..\\\\aug_dir'\n",
    "img_dir = os.path.join(aug_dir, 'img_dir')    \n",
    "\n",
    "\n",
    "# Choose a class\n",
    "for img_class in class_list:\n",
    "    \n",
    "    # creazione cartelle contenenti le augmented\n",
    "    dst_class_dir = os.path.join(img_dir, img_class)\n",
    "    os.mkdir(dst_class_dir)\n",
    "    \n",
    "    \n",
    "    # list all images in that directory\n",
    "    src_class_dir = os.path.join('..\\\\skin-cancer-mnist-ham10000\\\\HAM10000_images_part_1', img_class)\n",
    "    img_list = os.listdir(src_class_dir) # qui le immagini sono già nelle proprie cartelle\n",
    "    print('#img: '+ str(len(img_list)))\n",
    "    \n",
    "    # per ogni immagine devo creare le augmented\n",
    "    for fname in img_list:\n",
    "        fpath = os.path.join(src_class_dir, fname)\n",
    "        img = Image.open(fpath)  # this is a PIL image\n",
    "        x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "        x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "        \n",
    "        prefix = fname.replace(\".jpg\", \"\")\n",
    "        # the .flow() command below generates batches of randomly transformed images\n",
    "        # and saves the results to the `preview/` directory\n",
    "        i = 0\n",
    "        for batch in datagen.flow(x, batch_size=1,\n",
    "                                  save_to_dir=dst_class_dir, save_prefix=prefix , save_format='jpg'):\n",
    "            i += 1\n",
    "            if i > aug_size[int(img_class)]:\n",
    "                break  # otherwise the generator would loop indefinitely\n",
    "        \n",
    "        del x\n",
    "        img.close()\n",
    "    # run the generator and create about 6000 augmented images\n",
    "    #for i in range(0,num_batches):\n",
    "    #\n",
    "    #    imgs, labels = next(aug_datagen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## sistemazione nuovo dataset\n",
    "aug_dir = '..\\\\aug_dir'\n",
    "img_dir = os.path.join(aug_dir, 'img_dir') \n",
    "classes = os.listdir(img_dir)\n",
    "\n",
    "columns = encoded.columns\n",
    "tupla = pd.DataFrame(columns=columns)\n",
    "index = 10015\n",
    "\n",
    "for elem1 in classes:\n",
    "    class_dir = os.path.join(img_dir, elem1)\n",
    "    images = os.listdir(class_dir)\n",
    "    for elem2 in images:\n",
    "        elem2 = elem2.replace('.jpg', '')\n",
    "        tupla = tupla.append({'image_id': elem2, 'dx': int(elem1)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvataggio (punto 1.2)\n",
    "#dataset.to_csv('Pokemon_clean.csv', index=False) \n",
    "encoded = encoded.append(tupla)\n",
    "encoded.to_csv('dataframe_completo.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# da provare\n",
    "X = encoded[\"pixels\"]\n",
    "y = pd.get_dummies(encoded[\"dx\"])\n",
    "#bag_class = ensemble()\n",
    "X = np.asarray(X.tolist())\n",
    "y = np.asarray(y)\n",
    "\n",
    "# splitting test and validation\n",
    "X_train, X_test1, y_train, y_test1 = train_test_split(X, y, test_size=0.30)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test1, y_test1, test_size=0.40)\n",
    "\n",
    "\n",
    "classifier = new_classifier3()\n",
    "# STUDIARE BENE IL FIT\n",
    "classifier.fit(X_train, y_train, epochs=1, batch_size=100, shuffle=True, validation_data=(X_val, y_val)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "datagen.fit(X_train)\n",
    "\n",
    "cl2 = new_classifier3()\n",
    "# fits the model on batches with real-time data augmentation:\n",
    "cl2.fit_generator(datagen.flow(X_train, y_train, batch_size=32),\n",
    "                    steps_per_epoch=len(X_train) / 32, epochs=1)\n",
    "\n",
    "# here's a more \"manual\" example\n",
    "for e in range(2):\n",
    "    print('Epoch', e)\n",
    "    batches = 0\n",
    "    for x_batch, y_batch in datagen.flow(X_train, y_train, batch_size=20):\n",
    "        cl2.fit(x_batch, y_batch)\n",
    "        batches += 1\n",
    "        if batches >= len(X_train) / 20:\n",
    "            print('fine batch ', batches)\n",
    "            # we need to break the loop by hand because\n",
    "            # the generator loops indefinitely\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "### inizializzazione classificatore sul disco\n",
    "cl1 = new_classifier1()\n",
    "cl1.save('classifier1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### TRAINING\n",
    "#- copia temporanea dataset completo tmp\n",
    "#- dichiarazione size per il batch\n",
    "#- fin quando dim(tmp) >= size:\n",
    "#    - prendo size campioni random da tmp\n",
    "#    - train_test_split dei campioni\n",
    "#    - se esiste carico il modello del classificatore\n",
    "#    - fit del modello con questi campioni:\n",
    "#        - carico le immagini\n",
    "#        - converto in array\n",
    "#        - fit()                                                              (DA STUDIARE BENE I PARAMETRI, https://keras.io/models/sequential/)\n",
    "#        - dealloco le immagini\n",
    "#    - salvo il modello (dovrebbe funzionare perché non lo dealloco mai)\n",
    "#    - cancello i campioni da tmp\n",
    "#    - aggiorno dim\n",
    "#    - dealloco tutto\n",
    "\n",
    "dataset = pd.read_csv(csv_completo, encoding = \"ISO-8859-1\")\n",
    "batch = 1000\n",
    "\n",
    "\n",
    "def my_train_batch(dataset=dataset, batch=500, classifier='classifier1.h5'):\n",
    "    size = len(dataset)\n",
    "    epoch=0\n",
    "    while size >= batch:\n",
    "        # To get n random rows \n",
    "        samples = dataset.sample(n = batch)\n",
    "        # splitting test and validation\n",
    "        X_train, X_test1, y_train, y_test1 = train_test_split(samples[\"image_id\"], samples[\"dx\"], test_size=0.30)\n",
    "        X_test, X_val, y_test, y_val = train_test_split(X_test1, y_test1, test_size=0.40)\n",
    "        del X_test1, y_test1\n",
    "        train_img = load_images(X_train)\n",
    "        test_img = load_images(X_test)  # NB non sono mai usate, pensare se servano oppure no\n",
    "        val_img = load_images(X_val)\n",
    "        cl1 = k.models.load_model('classifier1.h5')\n",
    "        cl1.fit(np.asarray(train_img), y_train, epochs=1, shuffle=True, validation_data=(np.asarray(val_img), y_val))\n",
    "        cl1.save('classifier1.h5')\n",
    "        del X_train, y_train, X_test, y_test, X_val, y_val, train_img, test_img, val_img, classifier\n",
    "        dataset = dataset.drop(samples.index)\n",
    "        size = len(dataset)\n",
    "        del samples\n",
    "        epoch = epoch + 1\n",
    "        print(\"fine epoca \" + epoch + \";\\trestano \" + size + \"campioni nel dataset\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 700 arrays: [array([[[157, 134, 216],\n        [154, 131, 215],\n        [153, 134, 215],\n        ...,\n        [162, 145, 212],\n        [161, 147, 211],\n        [162, 149, 217]],\n\n       [[157, 136, 215],\n        [...",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-03f3274a6ed7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmy_train_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-3606d4dfe0c3>\u001b[0m in \u001b[0;36mmy_train_batch\u001b[1;34m(dataset, batch, classifier)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mval_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mcl1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'classifier1.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mcl1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0mcl1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'classifier1.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1154\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    107\u001b[0m                 \u001b[1;34m'Expected to see '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' array(s), '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[1;34m'but instead got the following list of '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[0;32m    110\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             raise ValueError(\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 700 arrays: [array([[[157, 134, 216],\n        [154, 131, 215],\n        [153, 134, 215],\n        ...,\n        [162, 145, 212],\n        [161, 147, 211],\n        [162, 149, 217]],\n\n       [[157, 136, 215],\n        [..."
     ]
    }
   ],
   "source": [
    "samples = dataset.sample(n=2000)\n",
    "my_train_batch(dataset=samples, batch=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
