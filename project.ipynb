{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creazione split\n",
    "#feat_train, feat_test, target_train, target_test = train_test_split(X, y, test_size=0.20, random_state=28) \n",
    "#smote = SMOTENC(sampling_strategy='dict', categorical_features=['bkl', 'vasc'])\n",
    "#X_sm, y_sm = smote.fit_sample(X,y)\n",
    "#len(X)\n",
    "#len(X_sm)\n",
    "\n",
    "\n",
    "#print(type(X))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##############################\n",
    "########## TODO ##############\n",
    "##############################\n",
    "\n",
    "\n",
    "- provare il fit del bagging_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "#import matplotlib.pyplot as plt\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#import sklearn as skl\n",
    "import sklearn.preprocessing as preproc\n",
    "import numpy as np\n",
    "import keras as k\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, ZeroPadding2D, Flatten, Dense\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "#from time import time\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### variabili globali\n",
    "image_path = '..\\..\\skin-cancer-mnist-ham10000\\HAM10000_images_part_1'\n",
    "csv_path = 'HAM10000_metadata.csv'\n",
    "dataset = pd.read_csv(csv_path, encoding = \"ISO-8859-1\")\n",
    "labels = set(dataset[\"dx\"])\n",
    "cat_column=[\"dx\"]\n",
    "ohe = preproc.OneHotEncoder(sparse=False, handle_unknown=\"ignore\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## API varie\n",
    "# - carica in memoria immagini e label\n",
    "# - 1-hot encoding\n",
    "# - implementazione modello tramite ensamble (nel classificatore va incluso il numero di labels)\n",
    "# - train e test usando il bagging, ma prima bisogna fare SMOTE su vasc e bkl?\n",
    "\n",
    "\n",
    "\n",
    "# carica le immagini e le label\n",
    "def loadPictures(path, dataset):\n",
    "    X=[];\n",
    "    y=[];\n",
    "    for _,_,files in os.walk(path+\"/\"):\n",
    "        for file in files: \n",
    "            #print(file) \n",
    "            #if file.find('.jpg')<0:\n",
    "            #    continue     \n",
    "            img = cv2.imread(path+\"/\"+file)\n",
    "            X.append(img)\n",
    "            file = file.replace(\".jpg\", \"\") #tagliare il .jpg che non compare nel dataset\n",
    "            y.append(dataset[dataset['image_id'] == file]['dx'])  \n",
    "    return X,y\n",
    "\n",
    "\n",
    "def lab_encode(dataset):#Ho una sola colonna! \n",
    "    #label_encoders = {} \n",
    "    for col in cat_column:\n",
    "        new_le = preproc.LabelEncoder()\n",
    "        new_ds = dataset\n",
    "        new_ds[col] = new_le.fit_transform(dataset[col]) \n",
    "      #  label_encoders[col] = new_le \n",
    "    return new_ds\n",
    "\n",
    "def onehot_encode(dataset):\n",
    "    dataset_LabEncoded = lab_encode(dataset)\n",
    "    x=np.array([dataset_LabEncoded[\"dx\"]]) \n",
    "    y_onehot = ohe.fit_transform(x.reshape(-1,1))\n",
    "    return y_onehot\n",
    "\n",
    "\n",
    "# si prende tutto l'array di risultati\n",
    "def reverse_onehot(vector):\n",
    "    res = ohe.inverse_transform(vector)\n",
    "    return res\n",
    "\n",
    "\n",
    "# conteggio statistiche dataset\n",
    "def stats(dataset):\n",
    "    labels = set(dataset['dx'])\n",
    "    for label in labels:\n",
    "        print(str(label), ':\\t', str(dataset[dataset['dx']==label]['dx'].count()))\n",
    "        #print (label)\n",
    "\n",
    "        \n",
    "        \n",
    "def new_classifier():\n",
    "    input_img = Input(shape=(450, 600, 3))  # 3x600x450 image RGB \n",
    "    #print (tf.shape(input_img))\n",
    "    x = Conv2D(20, (5, 5), activation='relu', padding='same')(input_img) # 20x450x600\n",
    "    #print (tf.shape(x))\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x) # 20x225x300\n",
    "    #print (tf.shape(x))\n",
    "    x = Conv2D(140, (3, 3), activation='relu', padding='same')(x) # 140x75x100\n",
    "    #print (tf.shape(x))\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x) # 40x38x50\n",
    "    #print (tf.shape(x))\n",
    "    x = Conv2D(50, (3, 3), activation='relu', padding='same')(x) # 50x38x50\n",
    "    #print (tf.shape(x))\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x) # 50x19x25\n",
    "    #print (tf.shape(x))\n",
    "    x = Flatten()(x) # qui la x diventa monodimensionale\n",
    "    #print (tf.shape(x))\n",
    "    # da qualche parte ci va il numero delle label?\n",
    "    encoded = Dense(7, activation='softmax')(x)\n",
    "    classifier = k.models.Model(input_img, encoded)   #questo è il nostro base_estimator, compile configura per il training\n",
    "    classifier.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "    return classifier\n",
    "\n",
    "\n",
    "\n",
    "def ensemble():\n",
    "    #da settare \n",
    "    #    base_estimator il classificatore come CNN,\n",
    "    #    n_estimators,\n",
    "    #    max_samples è il numero di elementi nei training set,\n",
    "    #    bootstrap se lo vogliamo o no (booleano),\n",
    "    #    n_jobs per il parallelismo,\n",
    "    classifier = new_classifier()\n",
    "    bagging = BaggingClassifier(base_estimator=classifier, n_estimators=1, \n",
    "                                bootstrap=True, n_jobs=4)\n",
    "    return bagging\n",
    "\n",
    "\n",
    "\n",
    "def load2(path, dataset):\n",
    "    data = pd.DataFrame(dataset)\n",
    "    data.insert(7, \"image\", None)\n",
    "    #contiene dx, nome immagine e array pixel\n",
    "    for _,_,files in os.walk(path+\"/\"):\n",
    "        for filejpg in files:     \n",
    "            img = cv2.imread(path+\"/\"+filejpg)\n",
    "            file = filejpg.replace(\".jpg\", \"\") #tagliare il .jpg che non compare nel dataset\n",
    "            data[data['image_id'] == file]['image'] = img\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot insert image, already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-23207308e7aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-5b4c53e4dd0c>\u001b[0m in \u001b[0;36mload2\u001b[1;34m(path, dataset)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m     \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"image\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m     \u001b[1;31m#contiene dx, nome immagine e array pixel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfiles\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36minsert\u001b[1;34m(self, loc, column, value, allow_duplicates)\u001b[0m\n\u001b[0;32m   3471\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbroadcast\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3472\u001b[0m         self._data.insert(loc, column, value,\n\u001b[1;32m-> 3473\u001b[1;33m                           allow_duplicates=allow_duplicates)\n\u001b[0m\u001b[0;32m   3474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3475\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36minsert\u001b[1;34m(self, loc, item, value, allow_duplicates)\u001b[0m\n\u001b[0;32m   1147\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_duplicates\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1148\u001b[0m             \u001b[1;31m# Should this be a different kind of error??\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1149\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cannot insert {}, already exists'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot insert image, already exists"
     ]
    }
   ],
   "source": [
    "ds = load2(image_path, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_class = ensemble()\n",
    "dataset_size = len(X)\n",
    "reshaped = np.reshape(X,(dataset_size,-1))\n",
    "bag_class.fit(reshaped, y_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
