{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- dataset splittato 70 30\n",
    "- apro il 70\n",
    "- creo cl_batch e cl_fit\n",
    "- suddivido in train validation\n",
    "- runno il mio batch e alleno cl_batch sul 70\n",
    "- runno il fit classico e alleno cl_fit sul 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import sklearn as skl\n",
    "import sklearn.preprocessing as preproc\n",
    "import numpy as np\n",
    "import keras as k\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, ZeroPadding2D, Flatten, Dense, Dropout, MaxPool2D\n",
    "from keras.models import Sequential\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "#from time import time\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import BatchNormalization\n",
    "import math\n",
    "import shutil\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### variabili globali\n",
    "#### GIUSEPPE\n",
    "#image_path = '..\\skin-cancer-mnist-ham10000\\HAM10000_images_part_1'\n",
    "image_path = '..\\\\..\\\\Untitled_Folder'\n",
    "#### FABIO\n",
    "# image_path = '..\\skin-cancer-mnist-ham10000\\HAM10000_images_part_1'\n",
    "csv_path = 'HAM10000_metadata.csv'\n",
    "csv_completo = 'dataframe_completo.csv'\n",
    "csv_completo_2 = 'dataframe_completo_2.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train_batch(dataset, batch, epochs, classifier):\n",
    "    size = len(dataset)\n",
    "    epoch=0\n",
    "    cl1 = k.models.load_model(classifier)\n",
    "    # train del classificatore un batch alla volta\n",
    "    while size >= batch:\n",
    "        # To get n random rows \n",
    "        samples = dataset.sample(n = batch)\n",
    "        # splitting test and validation\n",
    "        X_train, X_val, y_train, y_val = train_test_split(samples[\"image_id\"], samples[\"dx\"], test_size=0.30)\n",
    "        \n",
    "        train_img = np.asarray(load_images(X_train))\n",
    "        val_img = np.asarray(load_images(X_val))\n",
    "        \n",
    "        #1hot encoder\n",
    "        enc = OneHotEncoder(sparse=False)\n",
    "        y_train_shaped = np.array(y_train).reshape(-1,1)\n",
    "        y_val_shaped = np.array(y_val).reshape(-1,1)\n",
    "        enc.fit(y_train_shaped)\n",
    "        y_train_one = enc.transform(y_train_shaped)\n",
    "        y_val_one = enc.transform(y_val_shaped)\n",
    "        \n",
    "        cl1.fit(train_img, y_train_one, epochs=epochs, shuffle=True, validation_data=(val_img, y_val_one), verbose=1)\n",
    "        \n",
    "        #pulizia memoria e reset per il prossimo ciclo\n",
    "        del X_train, y_train_one, X_val, y_val_one, train_img, val_img\n",
    "        dataset = dataset.drop(samples.index)\n",
    "        size = len(dataset)\n",
    "        del samples, enc\n",
    "        epoch = epoch + 1\n",
    "        print(\"fine epoca \" + str(epoch) + \";\\trestano \" + str(size) + \" campioni nel dataset\")\n",
    "        \n",
    "    ######## ULTIMO CICLO CHE FINISCE IL DATASET\n",
    "    samples = dataset.sample(n = size)\n",
    "    # splitting test and validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(samples[\"image_id\"], samples[\"dx\"], test_size=0.30)\n",
    "    \n",
    "    train_img = np.asarray(load_images(X_train))\n",
    "    val_img = np.asarray(load_images(X_val))\n",
    "    \n",
    "    #1hot encoder\n",
    "    enc = OneHotEncoder(sparse=False)\n",
    "    y_train_shaped = np.array(y_train).reshape(-1,1)\n",
    "    y_val_shaped = np.array(y_val).reshape(-1,1)\n",
    "    enc.fit(y_train_shaped)\n",
    "    y_train_one = enc.transform(y_train_shaped)\n",
    "    y_val_one = enc.transform(y_val_shaped)\n",
    "    \n",
    "    cl1.fit(train_img, y_train_one, epochs=epochs, shuffle=True, validation_data=(val_img, y_val_one), verbose=1)\n",
    "    \n",
    "    del X_train, y_train_one, X_val, y_val_one, train_img, val_img\n",
    "    dataset = dataset.drop(samples.index)\n",
    "    size = len(dataset)\n",
    "    del samples, enc\n",
    "    epoch = epoch + 1\n",
    "    print(\"fine ultima epoca\")\n",
    "\n",
    "    cl1.save(classifier)\n",
    "    return\n",
    "\n",
    "# dal dataframe ne estrae le immagini in un array, serve nel batch\n",
    "def load_images(array):\n",
    "    images = list()\n",
    "    #print(array.iloc[0])\n",
    "    for index in range(0,len(array)):\n",
    "        #print(index.type())\n",
    "        img_id = array.iloc[index]\n",
    "        elem = os.path.join(image_path, img_id)\n",
    "        elem = elem + \".jpg\"\n",
    "        img = cv2.imread(elem)\n",
    "        # TODO va fatto un reshape, non capisco perchÃ©\n",
    "        images.append(np.asarray(img)) #########################################################################################\n",
    "    return images\n",
    "\n",
    "def new_classifier3():\n",
    "    # Set the CNN model \n",
    "    input_shape = (150, 200, 3)\n",
    "    num_classes = 7\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(20, kernel_size=(5, 5),activation='relu',padding = 'Same', input_shape = input_shape))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(140,kernel_size=(3, 3), activation='relu',padding = 'Same'))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(50,kernel_size=(3, 3), activation='relu',padding = 'Same'))\n",
    "    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "#def kaggle_classifier():\n",
    "#    # Set the CNN model \n",
    "#    # my CNN architechture is In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Out\n",
    "#    input_shape = (150, 200, 3)\n",
    "#    num_classes = 7\n",
    "#    \n",
    "#    model = Sequential()\n",
    "#    model.add(Conv2D(16, kernel_size=(3, 3),activation='relu',padding = 'Same',input_shape=input_shape))\n",
    "#    model.add(Conv2D(16,kernel_size=(3, 3), activation='relu',padding = 'Same',))\n",
    "#    model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "#    model.add(Dropout(0.25))\n",
    "#    \n",
    "#    model.add(Conv2D(32, (3, 3), activation='relu',padding = 'Same'))\n",
    "#    model.add(Conv2D(32, (3, 3), activation='relu',padding = 'Same'))\n",
    "#    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "#    model.add(Dropout(0.40))\n",
    "#    \n",
    "#    model.add(Flatten())\n",
    "#    model.add(Dense(128, activation='relu'))\n",
    "#    model.add(Dropout(0.5))\n",
    "#    model.add(Dense(num_classes, activation='softmax'))\n",
    "#    model.summary()\n",
    "#    model.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "#    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - dataset splittato 70 30\n",
    "# - apro il 70\n",
    "dataset = pd.read_csv(csv_completo_2, encoding = \"ISO-8859-1\")\n",
    "\n",
    "mass = int(len(dataset)*0.7)\n",
    "train = dataset.sample(n=mass)\n",
    "train.to_csv('train07.csv', index=False)\n",
    "test = dataset.drop(train.index)\n",
    "test.to_csv('test03.csv', index=False)\n",
    "\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train07.csv\", encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 150, 200, 20)      1520      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 75, 100, 20)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 75, 100, 20)       80        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 75, 100, 140)      25340     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 37, 50, 140)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 37, 50, 140)       560       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 37, 50, 50)        63050     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 18, 25, 50)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 18, 25, 50)        200       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 18, 25, 50)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 22500)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               2880128   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 2,971,781\n",
      "Trainable params: 2,971,361\n",
      "Non-trainable params: 420\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# - creo cl_batch e cl_fit\n",
    "n_cl = new_classifier3()\n",
    "n_cl.save('cl_batch.h5')\n",
    "n_cl.save('cl_fit.h5')\n",
    "del n_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - carico in memoria le immagini\n",
    "# - suddivido in train validation\n",
    "# - 1hot encoding delle y\n",
    "images = np.asarray(load_images(train[\"image_id\"]))\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, train[\"dx\"], test_size=0.30)\n",
    "\n",
    "#1hot encoder\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "y_train_shaped = np.array(y_train).reshape(-1,1)\n",
    "y_val_shaped = np.array(y_val).reshape(-1,1)\n",
    "enc.fit(y_train_shaped)\n",
    "y_train_one = enc.transform(y_train_shaped)\n",
    "y_val_one = enc.transform(y_val_shaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23497 samples, validate on 10071 samples\n",
      "Epoch 1/5\n",
      "23497/23497 [==============================] - 2539s 108ms/step - loss: 0.3341 - val_loss: 1.4010\n",
      "Epoch 2/5\n",
      "23497/23497 [==============================] - 962s 41ms/step - loss: 0.2451 - val_loss: 0.3050\n",
      "Epoch 3/5\n",
      "23497/23497 [==============================] - 966s 41ms/step - loss: 0.2165 - val_loss: 0.4612\n",
      "Epoch 4/5\n",
      "23497/23497 [==============================] - 1046s 45ms/step - loss: 0.1952 - val_loss: 0.3335\n",
      "Epoch 5/5\n",
      "23497/23497 [==============================] - 1044s 44ms/step - loss: 0.1765 - val_loss: 0.6442\n"
     ]
    }
   ],
   "source": [
    "# FIT di cl_fit\n",
    "cl_name = \"cl_fit.h5\"\n",
    "cl_fit = k.models.load_model(cl_name)\n",
    "cl_fit.fit(X_train, y_train_one, epochs=5, shuffle=True, validation_data=(X_val, y_val_one), verbose=1)\n",
    "cl_fit.save(cl_name)\n",
    "\n",
    "del X_train, X_val, y_train, y_val, y_train_shaped, y_val_shaped, y_train_one, y_val_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3500 samples, validate on 1500 samples\n",
      "Epoch 1/5\n",
      "3500/3500 [==============================] - 131s 37ms/step - loss: 0.2116 - val_loss: 0.2419\n",
      "Epoch 2/5\n",
      "3500/3500 [==============================] - 145s 41ms/step - loss: 0.1515 - val_loss: 0.3860\n",
      "Epoch 3/5\n",
      "3500/3500 [==============================] - 144s 41ms/step - loss: 0.1235 - val_loss: 0.4172\n",
      "Epoch 4/5\n",
      "3500/3500 [==============================] - 148s 42ms/step - loss: 0.0953 - val_loss: 0.2461\n",
      "Epoch 5/5\n",
      "3500/3500 [==============================] - 147s 42ms/step - loss: 0.0772 - val_loss: 0.4013\n",
      "fine epoca 1;\trestano 28568 campioni nel dataset\n",
      "Train on 3500 samples, validate on 1500 samples\n",
      "Epoch 1/5\n",
      "3500/3500 [==============================] - 145s 42ms/step - loss: 0.2162 - val_loss: 0.2962\n",
      "Epoch 2/5\n",
      "3500/3500 [==============================] - 151s 43ms/step - loss: 0.1551 - val_loss: 0.4884\n",
      "Epoch 3/5\n",
      "3500/3500 [==============================] - 149s 43ms/step - loss: 0.1179 - val_loss: 0.5801\n",
      "Epoch 4/5\n",
      "3500/3500 [==============================] - 147s 42ms/step - loss: 0.0934 - val_loss: 0.3846\n",
      "Epoch 5/5\n",
      "3500/3500 [==============================] - 148s 42ms/step - loss: 0.0701 - val_loss: 0.3793\n",
      "fine epoca 2;\trestano 23568 campioni nel dataset\n",
      "Train on 3500 samples, validate on 1500 samples\n",
      "Epoch 1/5\n",
      "3500/3500 [==============================] - 151s 43ms/step - loss: 0.2185 - val_loss: 0.5282\n",
      "Epoch 2/5\n",
      "3500/3500 [==============================] - 148s 42ms/step - loss: 0.1512 - val_loss: 0.2350\n",
      "Epoch 3/5\n",
      "3500/3500 [==============================] - 149s 43ms/step - loss: 0.1142 - val_loss: 0.3544\n",
      "Epoch 4/5\n",
      "3500/3500 [==============================] - 148s 42ms/step - loss: 0.0846 - val_loss: 0.3496\n",
      "Epoch 5/5\n",
      "3500/3500 [==============================] - 148s 42ms/step - loss: 0.0652 - val_loss: 0.5097\n",
      "fine epoca 3;\trestano 18568 campioni nel dataset\n",
      "Train on 3500 samples, validate on 1500 samples\n",
      "Epoch 1/5\n",
      "3500/3500 [==============================] - 148s 42ms/step - loss: 0.2227 - val_loss: 0.2077\n",
      "Epoch 2/5\n",
      "3500/3500 [==============================] - 150s 43ms/step - loss: 0.1518 - val_loss: 0.6235\n",
      "Epoch 3/5\n",
      "3500/3500 [==============================] - 150s 43ms/step - loss: 0.1162 - val_loss: 0.4009\n",
      "Epoch 4/5\n",
      "3500/3500 [==============================] - 158s 45ms/step - loss: 0.0915 - val_loss: 0.2909\n",
      "Epoch 5/5\n",
      "3500/3500 [==============================] - 150s 43ms/step - loss: 0.0699 - val_loss: 0.8626\n",
      "fine epoca 4;\trestano 13568 campioni nel dataset\n",
      "Train on 3500 samples, validate on 1500 samples\n",
      "Epoch 1/5\n",
      "3500/3500 [==============================] - 151s 43ms/step - loss: 0.2147 - val_loss: 0.3251\n",
      "Epoch 2/5\n",
      "3500/3500 [==============================] - 152s 43ms/step - loss: 0.1544 - val_loss: 0.4675\n",
      "Epoch 3/5\n",
      "3500/3500 [==============================] - 154s 44ms/step - loss: 0.1163 - val_loss: 1.1884\n",
      "Epoch 4/5\n",
      "3500/3500 [==============================] - 168s 48ms/step - loss: 0.0920 - val_loss: 0.2851\n",
      "Epoch 5/5\n",
      "3500/3500 [==============================] - 168s 48ms/step - loss: 0.0689 - val_loss: 0.5411\n",
      "fine epoca 5;\trestano 8568 campioni nel dataset\n",
      "Train on 3500 samples, validate on 1500 samples\n",
      "Epoch 1/5\n",
      "3500/3500 [==============================] - 163s 47ms/step - loss: 0.2018 - val_loss: 0.8091\n",
      "Epoch 2/5\n",
      "3500/3500 [==============================] - 157s 45ms/step - loss: 0.1488 - val_loss: 0.5591\n",
      "Epoch 3/5\n",
      "3500/3500 [==============================] - 149s 43ms/step - loss: 0.1125 - val_loss: 0.2467\n",
      "Epoch 4/5\n",
      "3500/3500 [==============================] - 152s 43ms/step - loss: 0.0801 - val_loss: 0.2483\n",
      "Epoch 5/5\n",
      "3500/3500 [==============================] - 151s 43ms/step - loss: 0.0614 - val_loss: 0.3060\n",
      "fine epoca 6;\trestano 3568 campioni nel dataset\n",
      "Train on 2497 samples, validate on 1071 samples\n",
      "Epoch 1/5\n",
      "2497/2497 [==============================] - 107s 43ms/step - loss: 0.2165 - val_loss: 0.3258\n",
      "Epoch 2/5\n",
      "2497/2497 [==============================] - 108s 43ms/step - loss: 0.1461 - val_loss: 1.1325\n",
      "Epoch 3/5\n",
      "2497/2497 [==============================] - 108s 43ms/step - loss: 0.1088 - val_loss: 0.2439\n",
      "Epoch 4/5\n",
      "2497/2497 [==============================] - 108s 43ms/step - loss: 0.0766 - val_loss: 0.2668\n",
      "Epoch 5/5\n",
      "2497/2497 [==============================] - 108s 43ms/step - loss: 0.0615 - val_loss: 0.2115\n",
      "fine ultima epoca\n"
     ]
    }
   ],
   "source": [
    "# - FIT di cl_batch su train\n",
    "cl_name = \"cl_batch.h5\"\n",
    "my_train_batch(dataset=train, batch=5000, epochs=5, classifier = cl_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
